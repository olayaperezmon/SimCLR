{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "if not os.path.isfile('IFCB.csv.zip'):\n",
    "    print(\"CSV data do not exist. Downloading...\")\n",
    "    !wget -O IFCB.csv.zip \"https://unioviedo-my.sharepoint.com/:u:/g/personal/gonzalezgpablo_uniovi_es/EfsVLhFsYJpPjO0KZlpWUq0BU6LaqJ989Re4XzatS9aG4Q?download=1\"\n",
    "\n",
    "data = pd.read_csv('IFCB.csv.zip',compression='infer', header=0,sep=',',quotechar='\"')\n",
    "\n",
    "#Compute sample and year information\n",
    "data['year'] = data['Sample'].str[6:10].astype(str) #Compute the year\n",
    "samples=data.groupby('Sample').first()\n",
    "samples = samples[[\"year\"]]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "classcolumn = \"AutoClass\" #AutoClass means 51 classes\n",
    "yearstraining = ['2006','2007'] #Years to consider as training\n",
    "yearstest = ['2008'] #Years to consider as test\n",
    "\n",
    "samplestraining = list(samples[samples['year'].isin(yearstraining)].index) #Samples to consider for training\n",
    "samplestest = list(samples[samples['year'].isin(yearstest)].index) #Samples to consider for testing\n",
    "\n",
    "classes=np.unique(data[classcolumn])\n",
    "classes.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5ifcbdataset import H5IFCBDataset\n",
    "import os\n",
    "\n",
    "hdf5_files_path = \"/media/nas/pgonzalez/IFCB_HDF5/output/\"\n",
    "\n",
    "#files to load\n",
    "filestraining = [hdf5_files_path+s+'.hdf5' for s in samplestraining]\n",
    "filestest = [hdf5_files_path+s+'.hdf5' for s in samplestest]\n",
    "\n",
    "#check if file exists\n",
    "if not os.path.isfile('training.pkl'):\n",
    "  train_dset = H5IFCBDataset(filestraining,classes,classattribute=classcolumn, verbose=1,trainingset=False)\n",
    "  train_dset.save(\"training.pkl\")\n",
    "else:\n",
    "  train_dset = H5IFCBDataset([],classes,classattribute=classcolumn, verbose=1,trainingset=False)\n",
    "  train_dset.load(\"training.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightly.models.modules.heads import SimCLRProjectionHead\n",
    "from lightly.loss import NTXentLoss\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimCLRModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "        hidden_dim = resnet.fc.in_features\n",
    "        self.projection_head = SimCLRProjectionHead(hidden_dim, hidden_dim, 128)\n",
    "\n",
    "        self.criterion = NTXentLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(h)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x0, x1), _, _ = batch\n",
    "        z0 = self.forward(x0)\n",
    "        z1 = self.forward(x1)\n",
    "        loss = self.criterion(z0, z1)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(), lr=6e-2, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optim, max_epochs\n",
    "        )\n",
    "        return [optim], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "percentage = 0.1\n",
    "percentage = round(percentage*len(train_dset))\n",
    "train_labeled, train_unlabeled = torch.utils.data.random_split(train_dset, [percentage, len(train_dset)-percentage],generator=torch.Generator().manual_seed(42))\n",
    "print(len(train_labeled))\n",
    "print(len(train_unlabeled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 8\n",
    "batch_size = 256\n",
    "seed = 1\n",
    "max_epochs = 100\n",
    "input_size = 64\n",
    "\n",
    "pl.seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightly\n",
    "import torchvision\n",
    "\n",
    "gpus = 2 if torch.cuda.is_available() else 0\n",
    "collate_fn = lightly.data.SimCLRCollateFunction(\n",
    "    input_size=input_size,\n",
    "    vf_prob=0.5,\n",
    "    rr_prob=0.5,\n",
    "    cj_prob=0.0,\n",
    "    random_gray_scale=0.0\n",
    ")\n",
    "\n",
    "dataloader_train_simclr = torch.utils.data.DataLoader(\n",
    "    train_unlabeled,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "model = SimCLRModel()\n",
    "trainer = pl.Trainer(\n",
    "    strategy=\"dp\",max_epochs=max_epochs, gpus=gpus, progress_bar_refresh_rate=100\n",
    ")\n",
    "trainer.fit(model, dataloader_train_simclr)\n",
    "\n",
    "pretrained_resnet_backbone = model.backbone\n",
    "\n",
    "# you can also store the backbone and use it in another code\n",
    "state_dict = {\n",
    "    'resnet18_parameters': pretrained_resnet_backbone.state_dict()\n",
    "}\n",
    "torch.save(state_dict, 'model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('simclr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b3ee39196fb4b11621e12a1cbbabf835d491d149762c580d30150a3eb594650"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
