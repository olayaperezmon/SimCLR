{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "if not os.path.isfile('IFCB.csv.zip'):\n",
    "    print(\"CSV data do not exist. Downloading...\")\n",
    "    !wget -O IFCB.csv.zip \"https://unioviedo-my.sharepoint.com/:u:/g/personal/gonzalezgpablo_uniovi_es/EfsVLhFsYJpPjO0KZlpWUq0BU6LaqJ989Re4XzatS9aG4Q?download=1\"\n",
    "\n",
    "data = pd.read_csv('IFCB.csv.zip',compression='infer', header=0,sep=',',quotechar='\"')\n",
    "\n",
    "#Compute sample and year information\n",
    "data['year'] = data['Sample'].str[6:10].astype(str) #Compute the year\n",
    "samples=data.groupby('Sample').first()\n",
    "samples = samples[[\"year\"]]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "classcolumn = \"AutoClass\" #AutoClass means 51 classes\n",
    "yearstraining = ['2006','2007'] #Years to consider as training\n",
    "yearstest = ['2008'] #Years to consider as test\n",
    "\n",
    "samplestraining = list(samples[samples['year'].isin(yearstraining)].index) #Samples to consider for training\n",
    "samplestest = list(samples[samples['year'].isin(yearstest)].index) #Samples to consider for testing\n",
    "\n",
    "classes=np.unique(data[classcolumn])\n",
    "classes.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from h5ifcbdataset import H5IFCBDataset\n",
    "import torchvision\n",
    "\n",
    "hdf5_files_path = \"/media/nas/pgonzalez/IFCB_HDF5/output/\"\n",
    "\n",
    "#files to load\n",
    "filestraining = [hdf5_files_path+s+'.hdf5' for s in samplestraining]\n",
    "filestest = [hdf5_files_path+s+'.hdf5' for s in samplestest]\n",
    "\n",
    "#Define transofrmations\n",
    "train_transform = T.Compose([\n",
    "  T.Resize(size=64),\n",
    "  T.RandomResizedCrop(size=64),\n",
    "  T.RandomHorizontalFlip(),\n",
    "  T.ToTensor()\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "  T.Resize(size=64),\n",
    "  T.CenterCrop(size=64),\n",
    "  T.ToTensor()\n",
    "])\n",
    "\n",
    "#check if file exists\n",
    "if not os.path.isfile('training.pkl') and not os.path.isfile(\"validation.pkl\"):\n",
    "  train_dset = H5IFCBDataset(filestraining,classes,classattribute=classcolumn, verbose=1,trainingset=False,transform=train_transform)\n",
    "  train_dset.save(\"training.pkl\")\n",
    "  val_dset = H5IFCBDataset(filestest,classes,classattribute=classcolumn, verbose=1,trainingset=False,transform=val_transform)\n",
    "  val_dset.save(\"validation.pkl\")\n",
    "else:\n",
    "  train_dset = H5IFCBDataset([],classes,classattribute=classcolumn, verbose=1,trainingset=False,transform=train_transform)\n",
    "  train_dset.load(\"training.pkl\")\n",
    "  val_dset = H5IFCBDataset([],classes,classattribute=classcolumn, verbose=1,trainingset=False,transform=val_transform)\n",
    "  val_dset.load(\"validation.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "percentage = 0.1\n",
    "percentage = round(percentage*len(train_dset))\n",
    "train_labeled, train_unlabeled = torch.utils.data.random_split(train_dset, [percentage, len(train_dset)-percentage],generator=torch.Generator().manual_seed(42))\n",
    "print(len(train_labeled))\n",
    "print(len(train_unlabeled))\n",
    "print(len(val_dset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class IFCBDataset(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size, train_dset,val_dset):\n",
    "        super().__init__()\n",
    "        self.train_dset = train_dset\n",
    "        self.val_dset = val_dset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = 51\n",
    "\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dset, batch_size=self.batch_size, shuffle=True,num_workers=8)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dset, batch_size=self.batch_size,num_workers=8)\n",
    "\n",
    "#Get only a subset of validation for speed up\n",
    "reduced_val_dset,_ = torch.utils.data.random_split(val_dset, [20000,len(val_dset)-20000],generator=torch.Generator().manual_seed(42))\n",
    "print(len(reduced_val_dset))\n",
    "finetuning_ds = IFCBDataset(512,train_labeled,reduced_val_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torch.nn.functional import cross_entropy\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "class IFCBFineTuning(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # init a pretrained resnet\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        #COMENTAR ESTAS DOS LINEAS PARA HACER SIMPLEMENTE FINETUNING SIN CARGAR LOS PESOS APRENDIDOS CON SIMCLR\n",
    "        ckpt = torch.load('model.pth')\n",
    "        self.model.load_state_dict(ckpt['resnet18_parameters'],strict=False)\n",
    "        \n",
    "        self.model.fc = torch.nn.Linear(self.model.fc.in_features, 51)\n",
    "        # for param in self.model.parameters():\n",
    "        #     param.requires_grad = True\n",
    "        self.valid_acc = torchmetrics.Accuracy()\n",
    "        \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # return the loss given a batch: this has a computational graph attached to it: optimization\n",
    "        x, y,_ = batch\n",
    "        preds = self.model(x)\n",
    "        loss = cross_entropy(preds, y)\n",
    "        self.log('train_loss', loss)  # lightning detaches your loss graph and uses its value\n",
    "        return loss\n",
    "\n",
    "    \n",
    "\n",
    "    #validation step\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, _= batch\n",
    "        y_hat = self.model(x)\n",
    "        self.valid_acc(y_hat.cpu(), y.cpu())\n",
    "        self.log('valid_acc', self.valid_acc, on_step=True, on_epoch=True,prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # return optimizer\n",
    "        optimizer = Adam(self.model.parameters(), lr=1e-4)\n",
    "        return optimizer\n",
    "\n",
    "finetuning_model = IFCBFineTuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = 2 if torch.cuda.is_available() else 0\n",
    "trainer = pl.Trainer(strategy=\"dp\",gpus=gpus, max_epochs=50)\n",
    "trainer.fit(finetuning_model, finetuning_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('simclr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b3ee39196fb4b11621e12a1cbbabf835d491d149762c580d30150a3eb594650"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
